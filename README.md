# Public Data Trust Engine

A comprehensive data quality pipeline for analyzing Aadhaar datasets (Enrolment, Demographic, Biometric).

## ğŸ“Š Features

- **Auto-detect broken or suspicious fields** - Identifies columns with high zero rates, near-zero variance
- **Flag statistical outliers** - Uses IQR and Z-score methods per pincode/day
- **Detect duplicates** - Exact and near-duplicate detection
- **Generate reliability score** - 0-100 score with detailed penalty breakdown
- **Produce automated reports** - JSON, CSV, and Markdown outputs
- **Interactive dashboard** - Streamlit-based visualization

## ğŸ“ Project Structure

```
data_quality_engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py           # Package exports
â”‚   â”œâ”€â”€ data_loader.py        # CSV ingestion & logging
â”‚   â”œâ”€â”€ normalizer.py         # Date/pincode standardization
â”‚   â”œâ”€â”€ quality_checks.py     # All DQ checks (A-E)
â”‚   â”œâ”€â”€ scoring.py            # Reliability scoring (0-100)
â”‚   â””â”€â”€ reporter.py           # Output generation
â”œâ”€â”€ outputs/                   # Generated reports
â”‚   â”œâ”€â”€ anomalies_summary.json
â”‚   â”œâ”€â”€ anomalies_table.csv
â”‚   â””â”€â”€ data_quality_report.md
â”œâ”€â”€ run_pipeline.py           # CLI entrypoint
â”œâ”€â”€ dashboard.py              # Streamlit dashboard
â””â”€â”€ requirements.txt          # Dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd c:\data_quality_engine
pip install -r requirements.txt
```

### 2. Run the Pipeline

```bash
python run_pipeline.py --input-dir "c:\"
```

### 3. View Reports

Reports are saved to `outputs/`:

- `anomalies_summary.json` - Machine-readable summary
- `anomalies_table.csv` - Row-level anomaly flags
- `data_quality_report.md` - Human-readable report

### 4. Launch Dashboard (Optional)

```bash
streamlit run dashboard.py
```

## ğŸ“‹ Quality Checks

| Check                     | Description                                         |
| ------------------------- | --------------------------------------------------- |
| **A) Broken Fields**      | Detects columns with >90% zeros, near-zero variance |
| **B) Duplicates**         | Exact and near-duplicate detection                  |
| **C) Outliers**           | IQR + Z-score methods                               |
| **D) Temporal Integrity** | Missing dates, sudden volume changes                |
| **E) Cross-Dataset**      | Enrolment vs Update ratio validation                |

## ğŸ“ˆ Reliability Score

Score Formula (start at 100):

- Duplicate penalty: -(pct Ã— 2), max 20 pts
- Broken fields: -(count Ã— 5), max 25 pts
- Outliers: -(pct Ã— 1.5), max 15 pts
- Date errors: -(pct Ã— 2), max 15 pts
- Missing data: -(pct Ã— 1), max 15 pts
- Temporal gaps: -(days Ã— 0.5), max 10 pts

## ğŸ“ CLI Options

```bash
python run_pipeline.py --help

Options:
  --input-dir   Directory containing dataset folders
  --output-dir  Directory for output reports (default: ./outputs)
  -v, --verbose Enable debug logging
```

## ğŸ“¦ Exit Codes

- `0` - Data quality acceptable (score â‰¥ 70)
- `1` - Data quality concerns (score 50-69)
- `2` - Critical issues (score < 50)
